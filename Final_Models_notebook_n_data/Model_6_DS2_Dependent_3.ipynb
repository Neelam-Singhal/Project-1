{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Manual Updates to file:\r\n",
    "- Convert excel file with three tabs to 3 CSV files\r\n",
    "- Put dependent and independent variables in same file.\r\n",
    "- Update column names for easier modelling\r\n",
    "    - Replace space with _ to ensure easier readability\r\n",
    "\r\n",
    "## Data Preprocessing\r\n",
    "1. Convert all -1 to 0.\r\n",
    "2. Remove Skewness from the data\r\n",
    "\r\n",
    "## Data Analysis:\r\n",
    "1. Check number of unique values\r\n",
    "2. Check if missing values\r\n",
    "3. Check Std. Deviation, min, max, quantiles etc"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Variable Description\r\n",
    "    Var 1\tInteger (min=0, no max)\r\n",
    "    Var 2\tReal (min=0, no max)\r\n",
    "    Var 3\tInteger (min=0, no max)\r\n",
    "    Var 4\tReal (Negative value possible though unlikely, no max)\r\n",
    "    Var 5\tOne of 8 classes (so -1 here means not in that class)\r\n",
    "    Var 6\tOne of 12 classes (so -1 here means not in that class)\t\r\n",
    "    Var 7\tInteger\r\n",
    "    Var 8\tInteger\r\n",
    "    Var 9\tInteger\r\n",
    "    Var 10\tInteger\r\n",
    "    Var 11\tReal (no min or max)\r\n",
    "    Var 12\tReal (no min or max)\r\n",
    "    Var 13\tInteger (min=2, no max)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import pandas as pd\r\n",
    "import numpy as np\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "import seaborn as sns\r\n",
    "\r\n",
    "# To plot QQ plot\r\n",
    "import statsmodels.api as sm\r\n",
    "\r\n",
    "import warnings\r\n",
    "warnings.filterwarnings('ignore')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "# To view all the columns \r\n",
    "pd.set_option(\"display.max_columns\", None)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "data = pd.read_csv('DS2_Dependent_3_xl.csv')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "data.head()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   Var_1     Var_2  Var_3      Var_4  Var_5_Class_1  Var_5_Class_2  \\\n",
       "0    5.0  0.933060   63.0  11.566940           -1.0           -1.0   \n",
       "1    8.0  2.124317   18.0  10.375683           -1.0           -1.0   \n",
       "2   18.0  1.618852   53.0  10.881148           -1.0           -1.0   \n",
       "3   11.0  1.005464   12.0  11.494536           -1.0           -1.0   \n",
       "4   12.0  1.008197   11.0  10.274590           -1.0           -1.0   \n",
       "\n",
       "   Var_5_Class_3  Var_5_Class_4  Var_5_Class_5  Var_5_Class_6  Var_5_Class_7  \\\n",
       "0           -1.0           -1.0           -1.0           -1.0           -1.0   \n",
       "1           -1.0           -1.0           -1.0           -1.0            1.0   \n",
       "2           -1.0           -1.0           -1.0           -1.0            1.0   \n",
       "3           -1.0           -1.0           -1.0           -1.0           -1.0   \n",
       "4           -1.0           -1.0           -1.0           -1.0           -1.0   \n",
       "\n",
       "   Var_5_Class_8  Var_6_Class_1  Var_6_Class_2  Var_6_Class_3  Var_6_Class_4  \\\n",
       "0            1.0            1.0           -1.0           -1.0           -1.0   \n",
       "1           -1.0            1.0           -1.0           -1.0           -1.0   \n",
       "2           -1.0            1.0           -1.0           -1.0           -1.0   \n",
       "3            1.0            1.0           -1.0           -1.0           -1.0   \n",
       "4            1.0           -1.0           -1.0            1.0           -1.0   \n",
       "\n",
       "   Var_6_Class_5  Var_6_Class_6  Var_6_Class_7  Var_6_Class_8  Var_6_Class_9  \\\n",
       "0           -1.0           -1.0           -1.0           -1.0           -1.0   \n",
       "1           -1.0           -1.0           -1.0           -1.0           -1.0   \n",
       "2           -1.0           -1.0           -1.0           -1.0           -1.0   \n",
       "3           -1.0           -1.0           -1.0           -1.0           -1.0   \n",
       "4           -1.0           -1.0           -1.0           -1.0           -1.0   \n",
       "\n",
       "   Var_6_Class_10  Var_6_Class_11  Var_6_Class_12  Var_7  Var_8  Var_9  \\\n",
       "0            -1.0            -1.0            -1.0    1.0    0.0   -1.0   \n",
       "1            -1.0            -1.0            -1.0    1.0    2.0   -1.0   \n",
       "2            -1.0            -1.0            -1.0    1.0    0.0   -1.0   \n",
       "3            -1.0            -1.0            -1.0    1.0    0.0   -1.0   \n",
       "4            -1.0            -1.0            -1.0    2.0    0.0   -1.0   \n",
       "\n",
       "   Var_10    Var_11     Var_12  Var_13  Input_Data_Set_3  \n",
       "0    -1.0  19.29529  39.459274     2.0               1.0  \n",
       "1    -1.0  19.29529  39.459274     2.0               0.0  \n",
       "2    -1.0  19.29529  39.459274     2.0               0.0  \n",
       "3    -1.0  19.29529  39.459274     2.0               0.0  \n",
       "4    -1.0  19.29529  39.459274     2.0               1.0  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Var_1</th>\n",
       "      <th>Var_2</th>\n",
       "      <th>Var_3</th>\n",
       "      <th>Var_4</th>\n",
       "      <th>Var_5_Class_1</th>\n",
       "      <th>Var_5_Class_2</th>\n",
       "      <th>Var_5_Class_3</th>\n",
       "      <th>Var_5_Class_4</th>\n",
       "      <th>Var_5_Class_5</th>\n",
       "      <th>Var_5_Class_6</th>\n",
       "      <th>Var_5_Class_7</th>\n",
       "      <th>Var_5_Class_8</th>\n",
       "      <th>Var_6_Class_1</th>\n",
       "      <th>Var_6_Class_2</th>\n",
       "      <th>Var_6_Class_3</th>\n",
       "      <th>Var_6_Class_4</th>\n",
       "      <th>Var_6_Class_5</th>\n",
       "      <th>Var_6_Class_6</th>\n",
       "      <th>Var_6_Class_7</th>\n",
       "      <th>Var_6_Class_8</th>\n",
       "      <th>Var_6_Class_9</th>\n",
       "      <th>Var_6_Class_10</th>\n",
       "      <th>Var_6_Class_11</th>\n",
       "      <th>Var_6_Class_12</th>\n",
       "      <th>Var_7</th>\n",
       "      <th>Var_8</th>\n",
       "      <th>Var_9</th>\n",
       "      <th>Var_10</th>\n",
       "      <th>Var_11</th>\n",
       "      <th>Var_12</th>\n",
       "      <th>Var_13</th>\n",
       "      <th>Input_Data_Set_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.0</td>\n",
       "      <td>0.933060</td>\n",
       "      <td>63.0</td>\n",
       "      <td>11.566940</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>19.29529</td>\n",
       "      <td>39.459274</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8.0</td>\n",
       "      <td>2.124317</td>\n",
       "      <td>18.0</td>\n",
       "      <td>10.375683</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>19.29529</td>\n",
       "      <td>39.459274</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>18.0</td>\n",
       "      <td>1.618852</td>\n",
       "      <td>53.0</td>\n",
       "      <td>10.881148</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>19.29529</td>\n",
       "      <td>39.459274</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.0</td>\n",
       "      <td>1.005464</td>\n",
       "      <td>12.0</td>\n",
       "      <td>11.494536</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>19.29529</td>\n",
       "      <td>39.459274</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12.0</td>\n",
       "      <td>1.008197</td>\n",
       "      <td>11.0</td>\n",
       "      <td>10.274590</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>19.29529</td>\n",
       "      <td>39.459274</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data Analysis"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "# Check shape of the data\r\n",
    "data.shape"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(1649, 32)"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "# Check how many unique values are there is the data. Helps to understand which column can be set as categorical vs numeric\r\n",
    "data.nunique()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Var_1                 52\n",
       "Var_2                849\n",
       "Var_3                 85\n",
       "Var_4               1040\n",
       "Var_5_Class_1          2\n",
       "Var_5_Class_2          2\n",
       "Var_5_Class_3          2\n",
       "Var_5_Class_4          2\n",
       "Var_5_Class_5          1\n",
       "Var_5_Class_6          2\n",
       "Var_5_Class_7          2\n",
       "Var_5_Class_8          2\n",
       "Var_6_Class_1          2\n",
       "Var_6_Class_2          2\n",
       "Var_6_Class_3          2\n",
       "Var_6_Class_4          2\n",
       "Var_6_Class_5          2\n",
       "Var_6_Class_6          2\n",
       "Var_6_Class_7          2\n",
       "Var_6_Class_8          2\n",
       "Var_6_Class_9          2\n",
       "Var_6_Class_10         2\n",
       "Var_6_Class_11         2\n",
       "Var_6_Class_12         2\n",
       "Var_7                  6\n",
       "Var_8                 15\n",
       "Var_9                  2\n",
       "Var_10                 2\n",
       "Var_11                12\n",
       "Var_12                13\n",
       "Var_13                12\n",
       "Input_Data_Set_3       2\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "# Check if there are any null values\r\n",
    "data.isnull().sum()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Var_1               384\n",
       "Var_2               384\n",
       "Var_3               384\n",
       "Var_4               384\n",
       "Var_5_Class_1       384\n",
       "Var_5_Class_2       384\n",
       "Var_5_Class_3       384\n",
       "Var_5_Class_4       384\n",
       "Var_5_Class_5       384\n",
       "Var_5_Class_6       384\n",
       "Var_5_Class_7       384\n",
       "Var_5_Class_8       384\n",
       "Var_6_Class_1       384\n",
       "Var_6_Class_2       384\n",
       "Var_6_Class_3       384\n",
       "Var_6_Class_4       384\n",
       "Var_6_Class_5       384\n",
       "Var_6_Class_6       384\n",
       "Var_6_Class_7       384\n",
       "Var_6_Class_8       384\n",
       "Var_6_Class_9       384\n",
       "Var_6_Class_10      384\n",
       "Var_6_Class_11      384\n",
       "Var_6_Class_12      384\n",
       "Var_7               384\n",
       "Var_8               384\n",
       "Var_9               384\n",
       "Var_10              384\n",
       "Var_11              424\n",
       "Var_12              384\n",
       "Var_13              384\n",
       "Input_Data_Set_3    999\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data Preprocessing"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We need to convert all -1 to 0 to help machine understand it better"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "# Saving all the columns which have -1 and 0 values in data_columns variable\r\n",
    "data_columns = ['Var_5_Class_1', 'Var_5_Class_2','Var_5_Class_3', 'Var_5_Class_4', 'Var_5_Class_5', 'Var_5_Class_6','Var_5_Class_7', 'Var_5_Class_8', 'Var_6_Class_1', 'Var_6_Class_2', 'Var_6_Class_3', 'Var_6_Class_4', 'Var_6_Class_5', 'Var_6_Class_6','Var_6_Class_7', 'Var_6_Class_8', 'Var_6_Class_9', 'Var_6_Class_10','Var_6_Class_11', 'Var_6_Class_12', 'Var_9', 'Var_10']"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "# Creating a function to convert all -1 to 0s\r\n",
    "\r\n",
    "def convert_to_zero_quick(df):\r\n",
    "    for i in data_columns:\r\n",
    "        df[i] = df[i].replace(-1, 0, regex=True)\r\n",
    "        "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "convert_to_zero_quick(data)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Remove the null data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "# Drop these columns. Ideally we should replace these columns with mean median or mode. But don't have additional information for this\r\n",
    "data.drop(data[pd.isna(data.Input_Data_Set_3)].index, inplace=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "1649-999"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "650"
      ]
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "data.shape"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(650, 32)"
      ]
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Oversample minority class"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "data[data.Input_Data_Set_3==0].shape, data[data.Input_Data_Set_3==1].shape"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "((304, 32), (346, 32))"
      ]
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "from sklearn.utils import resample\r\n",
    "\r\n",
    "# separate minority and majority classes\r\n",
    "not_accept = data[data.Input_Data_Set_3==0]\r\n",
    "accept = data[data.Input_Data_Set_3==1]\r\n",
    "\r\n",
    "# upsample minority\r\n",
    "not_accept_upsampled = resample(not_accept,\r\n",
    "                          replace=True, # sample with replacement\r\n",
    "                          n_samples=len(accept), # match number in majority class\r\n",
    "                          random_state=27) # reproducible results\r\n",
    "\r\n",
    "# combine majority and upsampled minority\r\n",
    "data = pd.concat([not_accept_upsampled, accept])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "data[data.Input_Data_Set_3==0].shape, data[data.Input_Data_Set_3==1].shape"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "((346, 32), (346, 32))"
      ]
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "# Saving the preprocessed data in a seperate csv file so that we don't have to repeat above steps multiple times\r\n",
    "data.to_csv(\"DS2_Dependent_3_xl_upsample.csv\", index=False)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Preprocess the data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "# Capturing the cleaned data\r\n",
    "data = pd.read_csv('DS2_Dependent_3_xl_upsample.csv')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "data.shape"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(692, 32)"
      ]
     },
     "metadata": {},
     "execution_count": 19
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "data_numeric = ['Var_1', 'Var_2', 'Var_3', 'Var_4', 'Var_7', 'Var_8', 'Var_11', 'Var_12', 'Var_13']"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "-0.5 to 0.5 -> Symmetric </br>\r\n",
    "Less than -0.5 -> Negatively Skewed </br>\r\n",
    "More then 0.5 -> Positively Skewed </br></br>\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "source": [
    "# Function to idenfity skewness\r\n",
    "def data_skewness(df):\r\n",
    "    for i in data_numeric:\r\n",
    "        print(i, \":  \", df[i].skew())"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "source": [
    "data_skewness(data)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Var_1 :   1.4309546501592163\n",
      "Var_2 :   1.2643824722296044\n",
      "Var_3 :   4.599974360599052\n",
      "Var_4 :   -2.001896361449669\n",
      "Var_7 :   3.150275659485977\n",
      "Var_8 :   3.46272765366114\n",
      "Var_11 :   -2.1778443529705442\n",
      "Var_12 :   -1.0314180387002772\n",
      "Var_13 :   -0.026436015899691318\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "source": [
    "# Removing skewness. Taking root for positive skewed data and power for negatively skewed data\r\n",
    "data[\"Var_1\"] = np.sqrt(data[\"Var_1\"])\r\n",
    "data[\"Var_2\"] = np.sqrt(data[\"Var_2\"])\r\n",
    "data[\"Var_3\"] = np.cbrt(data[\"Var_3\"])\r\n",
    "data[\"Var_7\"] = np.cbrt(data[\"Var_7\"])\r\n",
    "data[\"Var_8\"] = np.cbrt(data[\"Var_8\"])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "source": [
    "data[\"Var_4\"] = np.power(data[\"Var_4\"], 6)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "source": [
    "#data[\"Var_11\"] = np.log(data[\"Var_11\"])\r\n",
    "#data[\"Var_12\"] = np.power(data[\"Var_12\"], 3)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "source": [
    "#data[\"Var_12\"] = np.power(data[\"Var_12\"], 3)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "source": [
    "data_skewness(data)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Var_1 :   0.15078218848750297\n",
      "Var_2 :   0.5517589714665083\n",
      "Var_3 :   0.4852530307796883\n",
      "Var_4 :   0.03950820638036453\n",
      "Var_7 :   2.5435323937260548\n",
      "Var_8 :   -0.17348787540419966\n",
      "Var_11 :   -2.1778443529705442\n",
      "Var_12 :   -1.0314180387002772\n",
      "Var_13 :   -0.026436015899691318\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "source": [
    "#data.Var_7.describe()\r\n",
    "data.Var_11.isna().sum()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "metadata": {},
     "execution_count": 29
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Model Building\r\n",
    "We are building RandomForest model as it is less impacted by outliers"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "source": [
    "model_data = data"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "source": [
    "# After model building, we identified various features which were less important to model building. So we removed those. \r\n",
    "\r\n",
    "#model_data = data.drop(['Var_5_Class_5', 'Var_5_Class_1', 'Var_5_Class_4', 'Var_5_Class_6', 'Var_5_Class_3', 'Var_5_Class_2'], axis=1)\r\n",
    "\r\n",
    "model_data = data.drop(['Var_5_Class_5', 'Var_5_Class_1', 'Var_5_Class_4', 'Var_5_Class_6', 'Var_5_Class_3'], axis=1)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We will be using two ways to test out model:\r\n",
    "- Train test split\r\n",
    "- Cross Validation"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "source": [
    "# Segregating data in independent and dependent variables\r\n",
    "\r\n",
    "X = model_data.drop(['Input_Data_Set_3'], axis=1)\r\n",
    "y = model_data['Input_Data_Set_3']"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "source": [
    "from sklearn.model_selection import train_test_split\r\n",
    "\r\n",
    "train_X, test_X, train_y, test_y = train_test_split(X, y, random_state=122, stratify = y)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "source": [
    "from sklearn.metrics import f1_score, accuracy_score\r\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\r\n",
    "from sklearn.model_selection import cross_val_score\r\n",
    "from numpy import mean\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Random Forest"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\r\n",
    "\r\n",
    "rf_model = RandomForestClassifier(n_estimators=11, random_state=22)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "source": [
    "# define evaluation procedure\r\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\r\n",
    "# evaluate model\r\n",
    "scores = cross_val_score(rf_model, X, y, scoring='accuracy', cv=cv, n_jobs=-1)\r\n",
    "# summarize performance\r\n",
    "print('Accuracy %.3f' % mean(scores))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Accuracy 0.749\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "source": [
    "# Fitting a single model using train test split\r\n",
    "rf_model.fit(train_X, train_y)\r\n",
    "\r\n",
    "rf_predicted_test = rf_model.predict(test_X)\r\n",
    "accuracy_rf = accuracy_score(rf_predicted_test, test_y)\r\n",
    "accuracy_rf"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.7052023121387283"
      ]
     },
     "metadata": {},
     "execution_count": 37
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "source": [
    "# Checking feature importance to identify features that can be eliminated.\r\n",
    "feature_importance_rf = pd.Series(rf_model.feature_importances_, index=train_X.columns)\r\n",
    "\r\n",
    "feature_importance_rf = feature_importance_rf.sort_values(ascending=False)\r\n",
    "\r\n",
    "feature_importance_rf"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Var_4             0.144930\n",
       "Var_2             0.144229\n",
       "Var_3             0.138705\n",
       "Var_1             0.107516\n",
       "Var_13            0.061335\n",
       "Var_11            0.059144\n",
       "Var_8             0.052979\n",
       "Var_12            0.043498\n",
       "Var_10            0.029140\n",
       "Var_7             0.021318\n",
       "Var_9             0.019280\n",
       "Var_6_Class_1     0.018473\n",
       "Var_6_Class_3     0.018232\n",
       "Var_6_Class_2     0.014197\n",
       "Var_6_Class_5     0.014119\n",
       "Var_6_Class_7     0.013765\n",
       "Var_6_Class_12    0.012943\n",
       "Var_6_Class_10    0.012123\n",
       "Var_6_Class_11    0.012069\n",
       "Var_5_Class_8     0.011721\n",
       "Var_6_Class_6     0.011082\n",
       "Var_6_Class_8     0.010463\n",
       "Var_6_Class_4     0.008706\n",
       "Var_5_Class_7     0.007933\n",
       "Var_6_Class_9     0.007735\n",
       "Var_5_Class_2     0.004367\n",
       "dtype: float64"
      ]
     },
     "metadata": {},
     "execution_count": 38
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Prediction\r\n",
    "model_name = rf_model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "source": [
    "predicted_data = pd.read_csv(\"v2_Inputs_for_Full_Predictions_3_imputed.csv\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "source": [
    "data_skewness(predicted_data)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Var_1 :   3.9019749056127604\n",
      "Var_2 :   1.2858898594763082\n",
      "Var_3 :   18.706620448485587\n",
      "Var_4 :   -1.284574825229781\n",
      "Var_7 :   39.19867678667724\n",
      "Var_8 :   9.854938149687225\n",
      "Var_11 :   2.131511019460773\n",
      "Var_12 :   -1.5937560903840868\n",
      "Var_13 :   -0.13667133273249232\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "source": [
    "# Removing skewness. Taking root for positive skewed data and power for negatively skewed data\r\n",
    "predicted_data[\"Var_1\"] = np.sqrt(predicted_data[\"Var_1\"])\r\n",
    "predicted_data[\"Var_2\"] = np.sqrt(predicted_data[\"Var_2\"])\r\n",
    "predicted_data[\"Var_3\"] = np.cbrt(predicted_data[\"Var_3\"])\r\n",
    "predicted_data[\"Var_7\"] = np.cbrt(predicted_data[\"Var_7\"])\r\n",
    "predicted_data[\"Var_8\"] = np.cbrt(predicted_data[\"Var_8\"])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "source": [
    "data_skewness(predicted_data)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Var_1 :   0.49872883873460444\n",
      "Var_2 :   0.3576557463036899\n",
      "Var_3 :   0.9441237516931893\n",
      "Var_4 :   -1.284574825229781\n",
      "Var_7 :   3.9417182051002477\n",
      "Var_8 :   0.3735575358600709\n",
      "Var_11 :   2.131511019460773\n",
      "Var_12 :   -1.5937560903840868\n",
      "Var_13 :   -0.13667133273249232\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "source": [
    "# After model building, we identified various features which were less important to model building. So we removed those. \r\n",
    "\r\n",
    "#model_data = data.drop(['Var_5_Class_5', 'Var_5_Class_1', 'Var_5_Class_4', 'Var_5_Class_6', 'Var_5_Class_3', 'Var_5_Class_2'], axis=1)\r\n",
    "\r\n",
    "model_data = predicted_data.drop(['Var_5_Class_5', 'Var_5_Class_1', 'Var_5_Class_4', 'Var_5_Class_6', 'Var_5_Class_3'], axis=1)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "source": [
    "predictions = rf_model.predict(model_data)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "source": [
    "predictions_probability = rf_model.predict_proba(model_data)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "source": [
    "predictions_probability_0 = predictions_probability[:,0]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "source": [
    "prediction_confidence = []\r\n",
    "for i in predictions_probability_0:\r\n",
    "    if (i > 0.7):\r\n",
    "        prediction_confidence.append(0)\r\n",
    "    elif (i < 0.3):\r\n",
    "        prediction_confidence.append(100)\r\n",
    "    else:\r\n",
    "        prediction_confidence.append(50)\r\n",
    "        \r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "source": [
    "output_file_df = pd.DataFrame({\r\n",
    "    \"DataSet\": 3,\r\n",
    "    \"PredictionSet\": 3,\r\n",
    "    \"Prediction\": predictions,\r\n",
    "   # \"Pred_proba_0\": predictions_probability_0,\r\n",
    "   # \"Pred_proba_1\": predictions_probability_1,\r\n",
    "    \"Confidence\": prediction_confidence\r\n",
    "})"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "source": [
    "output_file_df.to_csv(\"Final_File_DS2_Pred3.csv\", index=False)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.2",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.2 64-bit ('env': venv)"
  },
  "interpreter": {
   "hash": "e905ad718a08ed27c2c36c9ef4f1f873648da9e7705f623e5afe311501acb5e0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}