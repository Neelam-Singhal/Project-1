{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Manual Updates to file:\r\n",
    "- Convert excel file with three tabs to 3 CSV files\r\n",
    "- Put dependent and independent variables in same file.\r\n",
    "- Update column names for easier modelling\r\n",
    "    - Replace space with _ to ensure easier readability\r\n",
    "\r\n",
    "## Data Preprocessing\r\n",
    "1. Convert all -1 to 0.\r\n",
    "2. Remove Skewness from the data\r\n",
    "\r\n",
    "## Data Analysis:\r\n",
    "1. Check number of unique values\r\n",
    "2. Check if missing values\r\n",
    "3. Check Std. Deviation, min, max, quantiles etc"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Variable Description\r\n",
    "    Var 1\tInteger (min=0, no max)\r\n",
    "    Var 2\tReal (min=0, no max)\r\n",
    "    Var 3\tInteger (min=0, no max)\r\n",
    "    Var 4\tReal (Negative value possible though unlikely, no max)\r\n",
    "    Var 5\tOne of 8 classes (so -1 here means not in that class)\r\n",
    "    Var 6\tOne of 12 classes (so -1 here means not in that class)\t\r\n",
    "    Var 7\tInteger\r\n",
    "    Var 8\tInteger\r\n",
    "    Var 9\tInteger\r\n",
    "    Var 10\tInteger\r\n",
    "    Var 11\tReal (no min or max)\r\n",
    "    Var 12\tReal (no min or max)\r\n",
    "    Var 13\tInteger (min=2, no max)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import pandas as pd\r\n",
    "import numpy as np\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "import seaborn as sns\r\n",
    "\r\n",
    "# To plot QQ plot\r\n",
    "import statsmodels.api as sm\r\n",
    "\r\n",
    "import warnings\r\n",
    "warnings.filterwarnings('ignore')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "# To view all the columns \r\n",
    "pd.set_option(\"display.max_columns\", None)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "data = pd.read_csv('Input_Set_model_3.csv')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "data.head()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   Var_1     Var_2  Var_3     Var_4  Var_5_Class_1  Var_5_Class_2  \\\n",
       "0      5  0.933060     63  7.566940             -1             -1   \n",
       "1      8  2.124317     18  6.375683             -1             -1   \n",
       "2     18  1.618852     53  6.881148             -1             -1   \n",
       "3     11  1.005464     12  7.494536             -1             -1   \n",
       "4     12  1.008197     11  6.274590             -1             -1   \n",
       "\n",
       "   Var_5_Class_3  Var_5_Class_4  Var_5_Class_5  Var_5_Class_6  Var_5_Class_7  \\\n",
       "0             -1             -1             -1             -1             -1   \n",
       "1             -1             -1             -1             -1              1   \n",
       "2             -1             -1             -1             -1              1   \n",
       "3             -1             -1             -1             -1             -1   \n",
       "4             -1             -1             -1             -1             -1   \n",
       "\n",
       "   Var_5_Class_8  Var_6_Class_1  Var_6_Class_2  Var_6_Class_3  Var_6_Class_4  \\\n",
       "0              1              1             -1             -1             -1   \n",
       "1             -1              1             -1             -1             -1   \n",
       "2             -1              1             -1             -1             -1   \n",
       "3              1              1             -1             -1             -1   \n",
       "4              1             -1             -1              1             -1   \n",
       "\n",
       "   Var_6_Class_5  Var_6_Class_6  Var_6_Class_7  Var_6_Class_8  Var_6_Class_9  \\\n",
       "0             -1             -1             -1             -1             -1   \n",
       "1             -1             -1             -1             -1             -1   \n",
       "2             -1             -1             -1             -1             -1   \n",
       "3             -1             -1             -1             -1             -1   \n",
       "4             -1             -1             -1             -1             -1   \n",
       "\n",
       "   Var_6_Class_10  Var_6_Class_11  Var_6_Class_12  Var_7  Var_8  Var_9  \\\n",
       "0              -1              -1              -1      1      0     -1   \n",
       "1              -1              -1              -1      1      2     -1   \n",
       "2              -1              -1              -1      1      2     -1   \n",
       "3              -1              -1              -1      1      0     -1   \n",
       "4              -1              -1              -1      2      0     -1   \n",
       "\n",
       "   Var_10     Var_11     Var_12  Var_13  Input_Data_Set_3  \n",
       "0      -1  15.133771  16.395182       2               1.0  \n",
       "1      -1  15.133771  16.395182       2               0.0  \n",
       "2      -1  15.133771  16.395182       2               0.0  \n",
       "3      -1  15.133771  16.395182       2               0.0  \n",
       "4      -1  15.133771  16.395182       2               1.0  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Var_1</th>\n",
       "      <th>Var_2</th>\n",
       "      <th>Var_3</th>\n",
       "      <th>Var_4</th>\n",
       "      <th>Var_5_Class_1</th>\n",
       "      <th>Var_5_Class_2</th>\n",
       "      <th>Var_5_Class_3</th>\n",
       "      <th>Var_5_Class_4</th>\n",
       "      <th>Var_5_Class_5</th>\n",
       "      <th>Var_5_Class_6</th>\n",
       "      <th>Var_5_Class_7</th>\n",
       "      <th>Var_5_Class_8</th>\n",
       "      <th>Var_6_Class_1</th>\n",
       "      <th>Var_6_Class_2</th>\n",
       "      <th>Var_6_Class_3</th>\n",
       "      <th>Var_6_Class_4</th>\n",
       "      <th>Var_6_Class_5</th>\n",
       "      <th>Var_6_Class_6</th>\n",
       "      <th>Var_6_Class_7</th>\n",
       "      <th>Var_6_Class_8</th>\n",
       "      <th>Var_6_Class_9</th>\n",
       "      <th>Var_6_Class_10</th>\n",
       "      <th>Var_6_Class_11</th>\n",
       "      <th>Var_6_Class_12</th>\n",
       "      <th>Var_7</th>\n",
       "      <th>Var_8</th>\n",
       "      <th>Var_9</th>\n",
       "      <th>Var_10</th>\n",
       "      <th>Var_11</th>\n",
       "      <th>Var_12</th>\n",
       "      <th>Var_13</th>\n",
       "      <th>Input_Data_Set_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>0.933060</td>\n",
       "      <td>63</td>\n",
       "      <td>7.566940</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>15.133771</td>\n",
       "      <td>16.395182</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8</td>\n",
       "      <td>2.124317</td>\n",
       "      <td>18</td>\n",
       "      <td>6.375683</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>15.133771</td>\n",
       "      <td>16.395182</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>18</td>\n",
       "      <td>1.618852</td>\n",
       "      <td>53</td>\n",
       "      <td>6.881148</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>15.133771</td>\n",
       "      <td>16.395182</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11</td>\n",
       "      <td>1.005464</td>\n",
       "      <td>12</td>\n",
       "      <td>7.494536</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>15.133771</td>\n",
       "      <td>16.395182</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12</td>\n",
       "      <td>1.008197</td>\n",
       "      <td>11</td>\n",
       "      <td>6.274590</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>15.133771</td>\n",
       "      <td>16.395182</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data Analysis"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "# Check shape of the data\r\n",
    "data.shape"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(651, 32)"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "# Check how many unique values are there is the data. Helps to understand which column can be set as categorical vs numeric\r\n",
    "data.nunique()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Var_1                52\n",
       "Var_2               526\n",
       "Var_3                83\n",
       "Var_4               573\n",
       "Var_5_Class_1         2\n",
       "Var_5_Class_2         2\n",
       "Var_5_Class_3         2\n",
       "Var_5_Class_4         2\n",
       "Var_5_Class_5         1\n",
       "Var_5_Class_6         2\n",
       "Var_5_Class_7         2\n",
       "Var_5_Class_8         2\n",
       "Var_6_Class_1         2\n",
       "Var_6_Class_2         2\n",
       "Var_6_Class_3         2\n",
       "Var_6_Class_4         2\n",
       "Var_6_Class_5         2\n",
       "Var_6_Class_6         2\n",
       "Var_6_Class_7         2\n",
       "Var_6_Class_8         2\n",
       "Var_6_Class_9         2\n",
       "Var_6_Class_10        2\n",
       "Var_6_Class_11        2\n",
       "Var_6_Class_12        2\n",
       "Var_7                 4\n",
       "Var_8                14\n",
       "Var_9                 2\n",
       "Var_10                2\n",
       "Var_11                8\n",
       "Var_12                9\n",
       "Var_13                8\n",
       "Input_Data_Set_3      2\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "data.shape"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(651, 32)"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "# Check if there are any null values\r\n",
    "data.isnull().sum()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Var_1                0\n",
       "Var_2                0\n",
       "Var_3                0\n",
       "Var_4                0\n",
       "Var_5_Class_1        0\n",
       "Var_5_Class_2        0\n",
       "Var_5_Class_3        0\n",
       "Var_5_Class_4        0\n",
       "Var_5_Class_5        0\n",
       "Var_5_Class_6        0\n",
       "Var_5_Class_7        0\n",
       "Var_5_Class_8        0\n",
       "Var_6_Class_1        0\n",
       "Var_6_Class_2        0\n",
       "Var_6_Class_3        0\n",
       "Var_6_Class_4        0\n",
       "Var_6_Class_5        0\n",
       "Var_6_Class_6        0\n",
       "Var_6_Class_7        0\n",
       "Var_6_Class_8        0\n",
       "Var_6_Class_9        0\n",
       "Var_6_Class_10       0\n",
       "Var_6_Class_11       0\n",
       "Var_6_Class_12       0\n",
       "Var_7                0\n",
       "Var_8                0\n",
       "Var_9                0\n",
       "Var_10               0\n",
       "Var_11              14\n",
       "Var_12               0\n",
       "Var_13               0\n",
       "Input_Data_Set_3     1\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "# Drop these columns. Ideally we should replace these columns with mean median or mode. But don't have additional information for this\r\n",
    "data.drop(data[pd.isna(data.Var_11)].index, inplace=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "data.shape"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(637, 32)"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data Preprocessing"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We need to convert all -1 to 0 to help machine understand it better"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "# Saving all the columns which have -1 and 0 values in data_columns variable\r\n",
    "data_columns = ['Var_5_Class_1', 'Var_5_Class_2','Var_5_Class_3', 'Var_5_Class_4', 'Var_5_Class_5', 'Var_5_Class_6','Var_5_Class_7', 'Var_5_Class_8', 'Var_6_Class_1', 'Var_6_Class_2', 'Var_6_Class_3', 'Var_6_Class_4', 'Var_6_Class_5', 'Var_6_Class_6','Var_6_Class_7', 'Var_6_Class_8', 'Var_6_Class_9', 'Var_6_Class_10','Var_6_Class_11', 'Var_6_Class_12', 'Var_9', 'Var_10']"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "# Creating a function to convert all -1 to 0s\r\n",
    "\r\n",
    "def convert_to_zero(df):\r\n",
    "    for i in data_columns:\r\n",
    "        for j in range(len(df)):\r\n",
    "            if (df[i][j] == -1):\r\n",
    "                df[i][j] = 0\r\n",
    "            "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "convert_to_zero(data)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "data.Input_Data_Set_3.value_counts()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "1.0    342\n",
       "0.0    295\n",
       "Name: Input_Data_Set_3, dtype: int64"
      ]
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Oversample minority class"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "data[data.Input_Data_Set_3==0].shape, data[data.Input_Data_Set_3==1].shape"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "((295, 32), (342, 32))"
      ]
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "from sklearn.utils import resample\r\n",
    "\r\n",
    "# separate minority and majority classes\r\n",
    "not_accept = data[data.Input_Data_Set_3==0]\r\n",
    "accept = data[data.Input_Data_Set_3==1]\r\n",
    "\r\n",
    "# upsample minority\r\n",
    "not_accept_upsampled = resample(not_accept,\r\n",
    "                          replace=True, # sample with replacement\r\n",
    "                          n_samples=len(accept), # match number in majority class\r\n",
    "                          #n_samples=2500000,\r\n",
    "                          random_state=27) # reproducible results\r\n",
    "\r\n",
    "# combine majority and upsampled minority\r\n",
    "data = pd.concat([not_accept_upsampled, accept])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "data[data.Input_Data_Set_3==0].shape, data[data.Input_Data_Set_3==1].shape"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "((342, 32), (342, 32))"
      ]
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "# Saving the preprocessed data in a seperate csv file so that we don't have to repeat above steps multiple times\r\n",
    "data.to_csv(\"Input_Set_3_upsample.csv\", index=False)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Preprocess the data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "# Capturing the cleaned data\r\n",
    "data = pd.read_csv('Input_Set_3_upsample.csv')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "data.shape"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(684, 32)"
      ]
     },
     "metadata": {},
     "execution_count": 20
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "source": [
    "# Creating a variable for numeric data\r\n",
    "data_numeric = ['Var_1', 'Var_2', 'Var_3', 'Var_4', 'Var_7', 'Var_8', 'Var_11', 'Var_12', 'Var_13']"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "-0.5 to 0.5 -> Symmetric </br>\r\n",
    "Less than -0.5 -> Negatively Skewed </br>\r\n",
    "More then 0.5 -> Positively Skewed </br></br>\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "source": [
    "# Function to idenfity skewness\r\n",
    "def data_skewness(df):\r\n",
    "    for i in data_numeric:\r\n",
    "        print(i, \":  \", df[i].skew())"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "source": [
    "data_skewness(data)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Var_1 :   1.4259445663557886\n",
      "Var_2 :   1.2619211285902883\n",
      "Var_3 :   4.5776283593422145\n",
      "Var_4 :   -2.001079314276153\n",
      "Var_7 :   3.210098082179848\n",
      "Var_8 :   3.2999263026221857\n",
      "Var_11 :   0.2524942974474454\n",
      "Var_12 :   0.267850340485727\n",
      "Var_13 :   -0.027995914651378892\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "source": [
    "# Removing skewness. Taking root for positive skewed data and power for negatively skewed data\r\n",
    "data[\"Var_1\"] = np.sqrt(data[\"Var_1\"])\r\n",
    "data[\"Var_2\"] = np.sqrt(data[\"Var_2\"])\r\n",
    "data[\"Var_3\"] = np.cbrt(data[\"Var_3\"])\r\n",
    "data[\"Var_7\"] = np.cbrt(data[\"Var_7\"])\r\n",
    "data[\"Var_8\"] = np.cbrt(data[\"Var_8\"])\r\n",
    "data[\"Var_12\"] = np.power(data[\"Var_12\"], 2)\r\n",
    "data[\"Var_4\"] = np.power(data[\"Var_4\"], 6)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "source": [
    "data_skewness(data)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Var_1 :   0.15287721840159382\n",
      "Var_2 :   0.5543573175245293\n",
      "Var_3 :   0.5129061407536686\n",
      "Var_4 :   0.5815776812037916\n",
      "Var_7 :   2.6219113083699614\n",
      "Var_8 :   -0.18364151819323887\n",
      "Var_11 :   0.2524942974474454\n",
      "Var_12 :   0.5746641537484819\n",
      "Var_13 :   -0.027995914651378892\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Model Building\r\n",
    "We are building RandomForest model as it is less impacted by outliers"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "source": [
    "# Saving data so far in a seperate DF\r\n",
    "model_data = data"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "source": [
    "# After model building, we identified various features which were less important to model building. So we removed those. \r\n",
    "model_data = data.drop(['Var_5_Class_5', 'Var_5_Class_1', 'Var_5_Class_4'], axis=1)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "source": [
    "# Segregating data in independent and dependent variables\r\n",
    "\r\n",
    "X = model_data.drop(['Input_Data_Set_3'], axis=1)\r\n",
    "y = model_data['Input_Data_Set_3']"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We will be using two ways to test out model:\r\n",
    "- Train test split\r\n",
    "- Cross Validation"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "source": [
    "\r\n",
    "from sklearn.model_selection import train_test_split\r\n",
    "\r\n",
    "train_X, test_X, train_y, test_y = train_test_split(X, y, random_state=122, stratify = y)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "source": [
    "from sklearn.metrics import f1_score, accuracy_score\r\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\r\n",
    "from sklearn.model_selection import cross_val_score\r\n",
    "from numpy import mean\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Random Forest"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\r\n",
    "\r\n",
    "rf_model = RandomForestClassifier(n_estimators=10, random_state=2)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "source": [
    "# define evaluation procedure\r\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\r\n",
    "# evaluate model\r\n",
    "scores = cross_val_score(rf_model, X, y, scoring='accuracy', cv=cv, n_jobs=-1)\r\n",
    "# summarize performance\r\n",
    "print('Accuracy %.3f' % mean(scores))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Accuracy 0.745\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "source": [
    "# Fitting a single model using train test split\r\n",
    "rf_model.fit(train_X, train_y)\r\n",
    "\r\n",
    "rf_predicted_test = rf_model.predict(test_X)\r\n",
    "accuracy_rf = accuracy_score(rf_predicted_test, test_y)\r\n",
    "accuracy_rf"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.7134502923976608"
      ]
     },
     "metadata": {},
     "execution_count": 33
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "source": [
    "# Checking feature importance to identify features that can be eliminated.\r\n",
    "feature_importance_rf = pd.Series(rf_model.feature_importances_, index=train_X.columns)\r\n",
    "\r\n",
    "feature_importance_rf = feature_importance_rf.sort_values(ascending=False)\r\n",
    "\r\n",
    "feature_importance_rf"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Var_2             0.136528\n",
       "Var_3             0.120230\n",
       "Var_1             0.111470\n",
       "Var_4             0.098735\n",
       "Var_8             0.080266\n",
       "Var_13            0.061487\n",
       "Var_9             0.055063\n",
       "Var_10            0.053698\n",
       "Var_12            0.039379\n",
       "Var_11            0.034413\n",
       "Var_5_Class_8     0.021901\n",
       "Var_7             0.021316\n",
       "Var_6_Class_2     0.016317\n",
       "Var_6_Class_12    0.015967\n",
       "Var_6_Class_7     0.014912\n",
       "Var_6_Class_1     0.014163\n",
       "Var_6_Class_9     0.012657\n",
       "Var_5_Class_7     0.012188\n",
       "Var_6_Class_5     0.011814\n",
       "Var_6_Class_8     0.011190\n",
       "Var_6_Class_4     0.010416\n",
       "Var_6_Class_11    0.009739\n",
       "Var_6_Class_3     0.008786\n",
       "Var_6_Class_6     0.008276\n",
       "Var_6_Class_10    0.007192\n",
       "Var_5_Class_2     0.005702\n",
       "Var_5_Class_6     0.005215\n",
       "Var_5_Class_3     0.000981\n",
       "dtype: float64"
      ]
     },
     "metadata": {},
     "execution_count": 34
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Prediction\r\n",
    "model_name = rf_model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "source": [
    "predicted_data = pd.read_csv(\"v2_Inputs_for_Full_Predictions_3_imputed.csv\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "source": [
    "data_skewness(predicted_data)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Var_1 :   3.9019749056127604\n",
      "Var_2 :   1.2858898594763082\n",
      "Var_3 :   18.706620448485587\n",
      "Var_4 :   -1.284574825229781\n",
      "Var_7 :   39.19867678667724\n",
      "Var_8 :   9.854938149687225\n",
      "Var_11 :   2.131511019460773\n",
      "Var_12 :   -1.5937560903840868\n",
      "Var_13 :   -0.13667133273249232\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "source": [
    "# Removing skewness. Taking root for positive skewed data and power for negatively skewed data\r\n",
    "predicted_data[\"Var_1\"] = np.sqrt(predicted_data[\"Var_1\"])\r\n",
    "predicted_data[\"Var_2\"] = np.sqrt(predicted_data[\"Var_2\"])\r\n",
    "predicted_data[\"Var_3\"] = np.cbrt(predicted_data[\"Var_3\"])\r\n",
    "predicted_data[\"Var_7\"] = np.cbrt(predicted_data[\"Var_7\"])\r\n",
    "predicted_data[\"Var_8\"] = np.cbrt(predicted_data[\"Var_8\"])\r\n",
    "predicted_data[\"Var_12\"] = np.power(predicted_data[\"Var_12\"], 2)\r\n",
    "#predicted_data[\"Var_4\"] = np.power(predicted_data[\"Var_4\"], 6)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "source": [
    "data_skewness(predicted_data)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Var_1 :   0.49872883873460444\n",
      "Var_2 :   0.3576557463036899\n",
      "Var_3 :   0.9441237516931893\n",
      "Var_4 :   -1.284574825229781\n",
      "Var_7 :   3.9417182051002477\n",
      "Var_8 :   0.3735575358600709\n",
      "Var_11 :   2.131511019460773\n",
      "Var_12 :   -1.4997594448550235\n",
      "Var_13 :   -0.13667133273249232\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "source": [
    "# After model building, we identified various features which were less important to model building. So we removed those. \r\n",
    "model_data = predicted_data.drop(['Var_5_Class_5', 'Var_5_Class_1', 'Var_5_Class_4'], axis=1)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "source": [
    "predictions = rf_model.predict(model_data)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "source": [
    "predictions_probability = rf_model.predict_proba(model_data)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "source": [
    "predictions_probability_0 = predictions_probability[:,0]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "source": [
    "prediction_confidence = []\r\n",
    "for i in predictions_probability_0:\r\n",
    "    if (i > 0.7):\r\n",
    "        prediction_confidence.append(0)\r\n",
    "    elif (i < 0.3):\r\n",
    "        prediction_confidence.append(100)\r\n",
    "    else:\r\n",
    "        prediction_confidence.append(50)\r\n",
    "        \r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "source": [
    "output_file_df = pd.DataFrame({\r\n",
    "    \"DataSet\": 3,\r\n",
    "    \"PredictionSet\": 3,\r\n",
    "    \"Prediction\": predictions,\r\n",
    "   # \"Pred_proba_0\": predictions_probability_0,\r\n",
    "   # \"Pred_proba_1\": predictions_probability_1,\r\n",
    "    \"Confidence\": prediction_confidence\r\n",
    "})"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "source": [
    "output_file_df.to_csv(\"Final_File_DS3_Pred3.csv\", index=False)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.2",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.2 64-bit"
  },
  "interpreter": {
   "hash": "a9ed818579b1ba32d90b6b7b1ccc8e0ff75013e2740df16cc93d983c2f1c495e"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}